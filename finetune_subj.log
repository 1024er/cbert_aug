+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ python -u finetune_dataset.py --task_name subj
Namespace(bert_model='bert-base-uncased', data_dir='datasets', do_lower_case=False, learning_rate=5e-05, max_seq_length=64, num_train_epochs=10.0, output_dir='aug_data', seed=42, task_name='stsa.binary', train_batch_size=32, warmup_proportion=0.1)
10/24/2019 16:40:42 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.
10/24/2019 16:40:45 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/24/2019 16:40:49 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba
10/24/2019 16:40:49 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpce5avkld
10/24/2019 16:40:53 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

10/24/2019 16:40:59 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
10/24/2019 16:41:25 - INFO - __main__ -   ***** Running training *****
10/24/2019 16:41:25 - INFO - __main__ -     Num examples = 31140
10/24/2019 16:41:25 - INFO - __main__ -     Batch size = 32
10/24/2019 16:41:25 - INFO - __main__ -     Num steps = 9731
Epoch:   0%|          | 0/10 [00:00<?, ?it/s]avg_loss: 3.002501301765442
avg_loss: 2.6321489214897156
avg_loss: 2.4933505702018737
avg_loss: 2.4827828073501585
avg_loss: 2.4085325717926027
avg_loss: 2.3700577092170714
avg_loss: 2.38023143529892
avg_loss: 2.3445087456703186
avg_loss: 2.2890095710754395
avg_loss: 2.2627751660346984
avg_loss: 2.2239297604560853
avg_loss: 2.164658269882202
avg_loss: 2.112579822540283
avg_loss: 2.142021613121033
avg_loss: 2.1704152488708495
avg_loss: 2.1657833886146545
avg_loss: 2.024079670906067
avg_loss: 2.090289664268494
avg_loss: 2.0870840167999267
Epoch:  10%|█         | 1/10 [06:29<58:23, 389.23s/it]avg_loss: 1.5070151901245117
avg_loss: 1.4038217043876648
avg_loss: 1.4169537043571472
avg_loss: 1.3882975053787232
avg_loss: 1.468806495666504
avg_loss: 1.3610171914100646
avg_loss: 1.4316205787658691
avg_loss: 1.3901313638687134
avg_loss: 1.2853389918804168
avg_loss: 1.4635883629322053
avg_loss: 1.4249585747718811
avg_loss: 1.3418354320526122
avg_loss: 1.3955275559425353
avg_loss: 1.2950031018257142
avg_loss: 1.383287831544876
avg_loss: 1.2969185650348662
avg_loss: 1.3305769789218902
avg_loss: 1.2313629078865052
avg_loss: 1.3458623743057252
Epoch:  20%|██        | 2/10 [12:57<51:52, 389.01s/it]avg_loss: 0.6429963076114654
avg_loss: 0.6213469862937927
avg_loss: 0.6202815335988998
avg_loss: 0.6916486382484436
avg_loss: 0.6623555660247803
avg_loss: 0.6404728305339813
avg_loss: 0.6569991856813431
avg_loss: 0.6938537293672562
avg_loss: 0.6498234689235687
avg_loss: 0.6451740419864654
avg_loss: 0.6734341734647751
avg_loss: 0.6875482791662216
avg_loss: 0.6381685662269593
avg_loss: 0.6713354754447937
avg_loss: 0.6160449105501175
avg_loss: 0.6911538308858871
avg_loss: 0.633777921795845
avg_loss: 0.6492578768730164
avg_loss: 0.6407470962405205
Epoch:  30%|███       | 3/10 [19:26<45:23, 389.03s/it]avg_loss: 0.27204143270850184
avg_loss: 0.2602607960999012
avg_loss: 0.2726223123073578
avg_loss: 0.28790842652320864
avg_loss: 0.2877562129497528
avg_loss: 0.2836748695373535
avg_loss: 0.3156432071328163
avg_loss: 0.28410994336009027
avg_loss: 0.3037267935276031
avg_loss: 0.2940799434483051
avg_loss: 0.3098851743340492
avg_loss: 0.3197346583008766
avg_loss: 0.31991717487573623
avg_loss: 0.3207520511746407
avg_loss: 0.3106842395663261
avg_loss: 0.2841997513175011
avg_loss: 0.3146902647614479
avg_loss: 0.31568029046058654
avg_loss: 0.3266257593035698
Epoch:  40%|████      | 4/10 [25:53<38:50, 388.38s/it]avg_loss: 0.1329217091202736
avg_loss: 0.13151939809322358
avg_loss: 0.14066958010196687
avg_loss: 0.13186664432287215
avg_loss: 0.13204937882721424
avg_loss: 0.13482004001736642
avg_loss: 0.13710838966071606
avg_loss: 0.14221289187669753
avg_loss: 0.13869472317397594
avg_loss: 0.13274274095892907
avg_loss: 0.13304286032915116
avg_loss: 0.134939581528306
avg_loss: 0.14708237558603288
avg_loss: 0.14714845396578313
avg_loss: 0.1376516705006361
avg_loss: 0.14685943007469177
avg_loss: 0.1354997757077217
avg_loss: 0.15192896284163
avg_loss: 0.13878932274878025
Epoch:  50%|█████     | 5/10 [32:21<32:21, 388.36s/it]avg_loss: 0.06316649906337261
avg_loss: 0.06239865742623806
avg_loss: 0.06521705906838178
avg_loss: 0.06320784952491522
avg_loss: 0.06638618253171444
avg_loss: 0.07055895317345857
avg_loss: 0.06793471612036228
avg_loss: 0.06768723245710134
avg_loss: 0.06256137508898973
avg_loss: 0.06947712548077106
avg_loss: 0.06838679000735283
avg_loss: 0.06566923309117556
avg_loss: 0.06986786924302578
avg_loss: 0.0664600621163845
avg_loss: 0.07616436585783959
avg_loss: 0.0739863432198763
avg_loss: 0.07718692548573017
avg_loss: 0.06970043692737818
avg_loss: 0.06844656333327294
Epoch:  60%|██████    | 6/10 [38:48<25:51, 387.85s/it]avg_loss: 0.03823384067043662
avg_loss: 0.03979595638811588
avg_loss: 0.04502802956849337
avg_loss: 0.04254504580050707
avg_loss: 0.046057292856276036
avg_loss: 0.04254351396113634
avg_loss: 0.04223353244364261
avg_loss: 0.04068698078393936
avg_loss: 0.03876552641391754
avg_loss: 0.04379283357411623
avg_loss: 0.044424363411962986
avg_loss: 0.04043907191604376
avg_loss: 0.03916029669344425
avg_loss: 0.036377187538892033
avg_loss: 0.04534252017736435
avg_loss: 0.04132672596722841
avg_loss: 0.047800592705607416
avg_loss: 0.0446542314440012
avg_loss: 0.04337126243859529
Epoch:  70%|███████   | 7/10 [45:16<19:23, 387.99s/it]avg_loss: 0.032816884461790326
avg_loss: 0.028441181387752295
avg_loss: 0.028731542453169823
avg_loss: 0.031152676064521073
avg_loss: 0.026603400576859712
avg_loss: 0.03185775378718972
avg_loss: 0.02637502782046795
avg_loss: 0.02763018649071455
avg_loss: 0.028736851289868356
avg_loss: 0.027281331419944762
avg_loss: 0.02831621205434203
avg_loss: 0.03039242831990123
avg_loss: 0.026554234512150287
avg_loss: 0.027099737655371426
avg_loss: 0.02995615215972066
avg_loss: 0.03022425003349781
avg_loss: 0.025967751126736403
avg_loss: 0.027385322730988263
avg_loss: 0.026847813464701175
Epoch:  80%|████████  | 8/10 [51:44<12:55, 387.90s/it]avg_loss: 0.020805933559313417
avg_loss: 0.019630982242524624
avg_loss: 0.018281223215162755
avg_loss: 0.022988997362554074
avg_loss: 0.020767254391685128
avg_loss: 0.023803320955485107
avg_loss: 0.02103971753269434
avg_loss: 0.020802758047357203
avg_loss: 0.02024362588301301
avg_loss: 0.022502893283963203
avg_loss: 0.019777686093002556
avg_loss: 0.021673589255660774
avg_loss: 0.021960584353655575
avg_loss: 0.021352158384397624
avg_loss: 0.021495292093604804
avg_loss: 0.02168962635099888
avg_loss: 0.02216933561488986
avg_loss: 0.019763037990778685
avg_loss: 0.02305266438052058
Epoch:  90%|█████████ | 9/10 [58:12<06:27, 387.81s/it]avg_loss: 0.016573777124285698
avg_loss: 0.01869789158925414
avg_loss: 0.016141280895099044
avg_loss: 0.016691572861745954
avg_loss: 0.018023939672857522
avg_loss: 0.015166376568377018
avg_loss: 0.018275243025273086
avg_loss: 0.017144820112735035
avg_loss: 0.016844989573583005
avg_loss: 0.015301192775368691
avg_loss: 0.015620265724137425
avg_loss: 0.015289771622046828
avg_loss: 0.015988740529865028
avg_loss: 0.01640735100954771
avg_loss: 0.017179688895121215
avg_loss: 0.015400505941361189
avg_loss: 0.015794238122180105
avg_loss: 0.016717679621651767
avg_loss: 0.017768791327252984
10/24/2019 17:46:00 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
10/24/2019 17:46:01 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
10/24/2019 17:46:01 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
10/24/2019 17:46:01 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
10/24/2019 17:46:02 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
10/24/2019 17:46:02 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
10/24/2019 17:46:03 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
10/24/2019 17:46:03 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
Epoch: 100%|██████████| 10/10 [1:04:38<00:00, 387.39s/it]Epoch: 100%|██████████| 10/10 [1:04:38<00:00, 387.87s/it]
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ python -u finetune_dataset.py --task_name subj
Namespace(bert_model='bert-base-uncased', data_dir='datasets', do_lower_case=False, learning_rate=5e-05, max_seq_length=64, num_train_epochs=10.0, output_dir='aug_data', seed=42, task_name='subj', train_batch_size=32, warmup_proportion=0.1)
10/25/2019 02:18:33 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.
10/25/2019 02:18:36 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/25/2019 02:18:39 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba
10/25/2019 02:18:39 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpdipuzzpv
10/25/2019 02:18:44 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

10/25/2019 02:18:50 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
10/25/2019 02:19:22 - INFO - __main__ -   ***** Running training *****
10/25/2019 02:19:22 - INFO - __main__ -     Num examples = 40500
10/25/2019 02:19:22 - INFO - __main__ -     Batch size = 32
10/25/2019 02:19:22 - INFO - __main__ -     Num steps = 12656
Epoch:   0%|          | 0/10 [00:00<?, ?it/s]avg_loss: 2.833844633102417
avg_loss: 2.56902713060379
avg_loss: 2.4530086755752563
avg_loss: 2.487256319522858
avg_loss: 2.478347146511078
avg_loss: 2.445440323352814
avg_loss: 2.332883131504059
avg_loss: 2.352031810283661
avg_loss: 2.3528224635124206
avg_loss: 2.2914636158943176
avg_loss: 2.2872705340385435
avg_loss: 2.3169104504585265
avg_loss: 2.2479163432121276
avg_loss: 2.2859868693351744
avg_loss: 2.2202763819694518
avg_loss: 2.228600878715515
avg_loss: 2.17468471288681
avg_loss: 2.2116239619255067
avg_loss: 2.1307595658302305
avg_loss: 2.130315239429474
avg_loss: 2.122562186717987
avg_loss: 2.0796123695373536
avg_loss: 2.1305424547195435
avg_loss: 2.111071252822876
avg_loss: 2.104948537349701
Epoch:  10%|█         | 1/10 [08:17<1:14:34, 497.13s/it]avg_loss: 1.4547154080867768
avg_loss: 1.4921906399726867
avg_loss: 1.4994610095024108
avg_loss: 1.5045586514472962
avg_loss: 1.4634498238563538
avg_loss: 1.5391560435295104
avg_loss: 1.5223528635501862
avg_loss: 1.4901990246772767
avg_loss: 1.5165768098831176
avg_loss: 1.4854433834552765
avg_loss: 1.4991616606712341
avg_loss: 1.4324973130226135
avg_loss: 1.5141554689407348
avg_loss: 1.4787080633640288
avg_loss: 1.413453416824341
avg_loss: 1.5028272914886474
avg_loss: 1.4590193903446198
avg_loss: 1.4517773032188415
avg_loss: 1.402006629705429
avg_loss: 1.4068842840194702
avg_loss: 1.392995537519455
avg_loss: 1.384720047712326
avg_loss: 1.4620081758499146
avg_loss: 1.4687091636657714
avg_loss: 1.4490267086029052
Epoch:  20%|██        | 2/10 [16:36<1:06:23, 497.89s/it]avg_loss: 0.7149847340583801
avg_loss: 0.7433906006813049
avg_loss: 0.7436145567893981
avg_loss: 0.7151446032524109
avg_loss: 0.7326258623600006
avg_loss: 0.7215860545635223
avg_loss: 0.7013872760534287
avg_loss: 0.7325228548049927
avg_loss: 0.7696830743551254
avg_loss: 0.7560790705680848
avg_loss: 0.7162584853172302
avg_loss: 0.7919469064474106
avg_loss: 0.7581051641702652
avg_loss: 0.7642374390363693
avg_loss: 0.7448937171697616
avg_loss: 0.7267289924621582
avg_loss: 0.7643599945306778
avg_loss: 0.801611704826355
avg_loss: 0.7242981338500977
avg_loss: 0.7496818047761917
avg_loss: 0.7273104393482208
avg_loss: 0.7539543634653092
avg_loss: 0.7548749488592148
avg_loss: 0.7305537271499634
avg_loss: 0.7493273425102234
Epoch:  30%|███       | 3/10 [24:57<58:10, 498.65s/it]  avg_loss: 0.33167022168636323
avg_loss: 0.3571309342980385
avg_loss: 0.3452304118871689
avg_loss: 0.35008210271596907
avg_loss: 0.3277091559767723
avg_loss: 0.3577996990084648
avg_loss: 0.3359138783812523
avg_loss: 0.37641227632761004
avg_loss: 0.35231547713279726
avg_loss: 0.3723397797346115
avg_loss: 0.3742025354504585
avg_loss: 0.35634840965271
avg_loss: 0.36460030257701875
avg_loss: 0.39636153787374495
avg_loss: 0.37640694618225096
avg_loss: 0.3543600654602051
avg_loss: 0.37714397996664045
avg_loss: 0.3705858743190765
avg_loss: 0.3805253219604492
avg_loss: 0.3769581687450409
avg_loss: 0.3856268674135208
avg_loss: 0.39551895290613176
avg_loss: 0.3930786818265915
avg_loss: 0.39123064577579497
avg_loss: 0.37946081221103667
Epoch:  40%|████      | 4/10 [33:17<49:54, 499.08s/it]avg_loss: 0.17348672598600387
avg_loss: 0.17166674867272377
avg_loss: 0.15689114078879357
avg_loss: 0.15951739579439164
avg_loss: 0.1783872973918915
avg_loss: 0.17776912346482276
avg_loss: 0.18513092041015625
avg_loss: 0.1886035259068012
avg_loss: 0.1783902971446514
avg_loss: 0.19070314154028892
avg_loss: 0.17625979632139205
avg_loss: 0.18331328481435777
avg_loss: 0.17256439477205276
avg_loss: 0.1796538007259369
avg_loss: 0.1815241700410843
avg_loss: 0.1799566274881363
avg_loss: 0.19982260942459107
avg_loss: 0.17137575328350066
avg_loss: 0.17734002366662024
avg_loss: 0.18928585052490235
avg_loss: 0.18046893209218978
avg_loss: 0.18755513355135917
avg_loss: 0.18726551607251168
avg_loss: 0.18302346169948577
avg_loss: 0.17790249109268189
Epoch:  50%|█████     | 5/10 [41:37<41:37, 499.42s/it]avg_loss: 0.09318204000592231
avg_loss: 0.08661426521837712
avg_loss: 0.08913303554058075
avg_loss: 0.08547591388225556
avg_loss: 0.08446647979319095
avg_loss: 0.09136272318661214
avg_loss: 0.09143144659698009
avg_loss: 0.08998616270720959
avg_loss: 0.08754940427839757
avg_loss: 0.0893469636887312
avg_loss: 0.09549598969519138
avg_loss: 0.0886211472004652
avg_loss: 0.09286643609404564
avg_loss: 0.09083690606057644
avg_loss: 0.08808655835688114
avg_loss: 0.08913338534533978
avg_loss: 0.09215982995927334
avg_loss: 0.09002646513283252
avg_loss: 0.09630147024989127
avg_loss: 0.09439483359456062
avg_loss: 0.09806466236710548
avg_loss: 0.10404421150684356
avg_loss: 0.10207996308803559
avg_loss: 0.09363956652581691
avg_loss: 0.09423780255019665
Epoch:  60%|██████    | 6/10 [49:57<33:17, 499.49s/it]avg_loss: 0.05733028318732977
avg_loss: 0.05376441404223442
avg_loss: 0.051531344186514616
avg_loss: 0.05700765628367663
avg_loss: 0.05690672241151333
avg_loss: 0.056236560940742496
avg_loss: 0.05320508789271116
avg_loss: 0.05618797574192286
avg_loss: 0.05608065739274025
avg_loss: 0.054452968798577785
avg_loss: 0.05393958881497383
avg_loss: 0.054426099471747875
avg_loss: 0.05863734796643257
avg_loss: 0.05681411150842905
avg_loss: 0.055804132744669915
avg_loss: 0.05378728177398443
avg_loss: 0.0591630532220006
avg_loss: 0.05860629994422197
avg_loss: 0.060364442244172097
avg_loss: 0.05555788770318031
avg_loss: 0.05634676687419415
avg_loss: 0.06293859362602233
avg_loss: 0.055039648115634915
avg_loss: 0.05534857057034969
avg_loss: 0.05722325589507818
Epoch:  70%|███████   | 7/10 [58:18<25:00, 500.11s/it]avg_loss: 0.040306642688810825
avg_loss: 0.034004409201443195
avg_loss: 0.03974319022148848
avg_loss: 0.036908339560031894
avg_loss: 0.03831965021789074
avg_loss: 0.03601493924856186
avg_loss: 0.036867426708340645
avg_loss: 0.0341346450522542
avg_loss: 0.03675453811883926
avg_loss: 0.037664075903594496
avg_loss: 0.038816789910197255
avg_loss: 0.038727146983146665
avg_loss: 0.03716208919882774
avg_loss: 0.036892177127301695
avg_loss: 0.03772344056516886
avg_loss: 0.037324119322001935
avg_loss: 0.0407803201675415
avg_loss: 0.03589952316135168
avg_loss: 0.03554542534053326
avg_loss: 0.035877642929553984
avg_loss: 0.03844025038182736
avg_loss: 0.03824749160557985
avg_loss: 0.037177741974592206
avg_loss: 0.03761873139068484
avg_loss: 0.039012438878417016
Epoch:  80%|████████  | 8/10 [1:06:42<16:42, 501.27s/it]avg_loss: 0.025585749950259924
avg_loss: 0.02708672923967242
avg_loss: 0.02776496222242713
avg_loss: 0.02534480744972825
avg_loss: 0.026367007847875358
avg_loss: 0.024278289433568717
avg_loss: 0.027637221347540618
avg_loss: 0.026977804619818926
avg_loss: 0.02498028427362442
avg_loss: 0.026707648430019618
avg_loss: 0.02419554805383086
avg_loss: 0.025340507198125126
avg_loss: 0.02428644010797143
avg_loss: 0.024130421131849288
avg_loss: 0.027882369440048932
avg_loss: 0.027329989951103927
avg_loss: 0.028531194254755973
avg_loss: 0.025424020141363145
avg_loss: 0.025949779748916626
avg_loss: 0.02298355970531702
avg_loss: 0.028715438190847636
avg_loss: 0.026065713185817005
avg_loss: 0.023314953576773406
avg_loss: 0.02647897806018591
avg_loss: 0.025146640483289958
Epoch:  90%|█████████ | 9/10 [1:15:06<08:21, 501.88s/it]avg_loss: 0.021126369927078486
avg_loss: 0.02095888812094927
avg_loss: 0.018930583484470843
avg_loss: 0.020732795856893062
avg_loss: 0.021500988900661468
avg_loss: 0.020363974645733834
avg_loss: 0.020077042039483784
avg_loss: 0.021658480782061815
avg_loss: 0.020536151602864264
avg_loss: 0.021464452724903823
avg_loss: 0.020431281980127097
avg_loss: 0.022367956452071666
avg_loss: 0.02056273691356182
avg_loss: 0.020581703018397093
avg_loss: 0.02014985593035817
avg_loss: 0.020497875567525626
avg_loss: 0.02056600237265229
avg_loss: 0.021686545852571727
avg_loss: 0.020036868955940007
avg_loss: 0.01997995814308524
avg_loss: 0.02189386706799269
avg_loss: 0.01866180025972426
avg_loss: 0.020447310525923967
avg_loss: 0.021059227157384158
avg_loss: 0.01987243866547942
10/25/2019 03:42:48 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
10/25/2019 03:42:49 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
10/25/2019 03:42:49 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
Epoch: 100%|██████████| 10/10 [1:23:27<00:00, 501.62s/it]Epoch: 100%|██████████| 10/10 [1:23:27<00:00, 500.70s/it]
