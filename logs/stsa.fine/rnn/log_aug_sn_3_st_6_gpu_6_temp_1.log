Namespace(bert_model='bert-base-uncased', data_dir='datasets', do_lower_case=False, gpu=6, learning_rate=5e-05, max_seq_length=64, num_train_epochs=9.0, output_dir='aug_data_3_6_6_1.0', sample_num=3, sample_ratio=6, seed=42, task_name='stsa.fine', temp=1.0, train_batch_size=32, warmup_proportion=0.1)
10/27/2019 01:49:06 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.
10/27/2019 01:49:08 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/27/2019 01:49:08 - INFO - __main__ -   *** Example ***
10/27/2019 01:49:08 - INFO - __main__ -   guid: train-1
10/27/2019 01:49:08 - INFO - __main__ -   tokens: [CLS] too smart to ignore but a little too smug ##ly superior to like , this could be a movie that ends up slapping its target audience in the face by shooting itself in the foot . [SEP]
10/27/2019 01:49:08 - INFO - __main__ -   init_ids: 101 2205 6047 2000 8568 2021 1037 2210 2205 20673 2135 6020 2000 2066 1010 2023 2071 2022 1037 3185 2008 4515 2039 22021 2049 4539 4378 1999 1996 2227 2011 5008 2993 1999 1996 3329 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/27/2019 01:49:08 - INFO - __main__ -   input_ids: 101 2205 103 2000 8568 103 1037 2210 103 20673 2135 103 2000 2066 1010 2023 2071 2022 1037 3185 2008 4515 2039 22021 2049 4539 4378 1999 1996 2227 2011 5008 103 1999 1996 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/27/2019 01:49:08 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/27/2019 01:49:08 - INFO - __main__ -   segment_ids: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/27/2019 01:49:08 - INFO - __main__ -   masked_lm_labels: -1 -1 6047 -1 -1 2021 -1 -1 2205 -1 -1 6020 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 2993 -1 -1 3329 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/27/2019 01:49:08 - INFO - __main__ -   *** Example ***
10/27/2019 01:49:08 - INFO - __main__ -   guid: train-2
10/27/2019 01:49:08 - INFO - __main__ -   tokens: [CLS] rai ##mi and his team could n ' t have done any better in bringing the story of spider - man to the big screen . [SEP]
10/27/2019 01:49:08 - INFO - __main__ -   init_ids: 101 15547 4328 1998 2010 2136 2071 1050 1005 1056 2031 2589 2151 2488 1999 5026 1996 2466 1997 6804 1011 2158 2000 1996 2502 3898 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/27/2019 01:49:08 - INFO - __main__ -   input_ids: 101 15547 4328 1998 2010 2136 2071 2136 1005 1056 2031 103 2151 2488 1999 5026 1996 2466 1997 6804 103 2158 2000 1996 2502 3898 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/27/2019 01:49:08 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/27/2019 01:49:08 - INFO - __main__ -   segment_ids: 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/27/2019 01:49:08 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 1998 -1 -1 -1 1050 -1 -1 -1 2589 -1 -1 -1 -1 -1 -1 -1 -1 1011 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/27/2019 01:49:13 - INFO - __main__ -   ***** Running training *****
10/27/2019 01:49:13 - INFO - __main__ -     Num examples = 7689
10/27/2019 01:49:13 - INFO - __main__ -     Batch size = 32
10/27/2019 01:49:13 - INFO - __main__ -     Num steps = 2162
constract vocabulary based on frequency
# train data: 7689
# test  data: 855
# vocab: 16581
# class: 5
before augment best acc:0.391855
Epoch:   0%|          | 0/9 [00:00<?, ?it/s]constract vocabulary based on frequency
# train data: 30746
# test  data: 855
# vocab: 21197
# class: 5
epoch 0 augment best acc:0.39276
Epoch:  11%|█         | 1/9 [16:21<2:10:48, 981.04s/it]constract vocabulary based on frequency
# train data: 30718
# test  data: 855
# vocab: 20371
# class: 5
epoch 1 augment best acc:0.415837
Epoch:  22%|██▏       | 2/9 [27:50<1:44:15, 893.66s/it]constract vocabulary based on frequency
# train data: 30726
# test  data: 855
# vocab: 19835
# class: 5
epoch 2 augment best acc:0.420362
Epoch:  33%|███▎      | 3/9 [38:44<1:22:10, 821.77s/it]constract vocabulary based on frequency
# train data: 30770
# test  data: 855
# vocab: 19831
# class: 5
epoch 3 augment best acc:0.40181
Epoch:  44%|████▍     | 4/9 [48:14<1:02:10, 746.18s/it]constract vocabulary based on frequency
# train data: 30752
# test  data: 855
# vocab: 19541
# class: 5
epoch 4 augment best acc:0.40543
Epoch:  56%|█████▌    | 5/9 [58:14<46:49, 702.31s/it]  constract vocabulary based on frequency
# train data: 30771
# test  data: 855
# vocab: 19711
# class: 5
epoch 5 augment best acc:0.392308
Epoch:  67%|██████▋   | 6/9 [1:07:14<32:41, 653.72s/it]constract vocabulary based on frequency
# train data: 30761
# test  data: 855
# vocab: 19560
# class: 5
epoch 6 augment best acc:0.408145
Epoch:  78%|███████▊  | 7/9 [1:16:37<20:52, 626.36s/it]constract vocabulary based on frequency
# train data: 30758
# test  data: 855
# vocab: 19549
# class: 5
epoch 7 augment best acc:0.401357
Epoch:  89%|████████▉ | 8/9 [1:24:32<09:40, 580.85s/it]constract vocabulary based on frequency
# train data: 30745
# test  data: 855
# vocab: 19514
# class: 5
epoch 8 augment best acc:0.371041
Epoch: 100%|██████████| 9/9 [1:31:45<00:00, 536.74s/it]Epoch: 100%|██████████| 9/9 [1:31:45<00:00, 611.77s/it]
