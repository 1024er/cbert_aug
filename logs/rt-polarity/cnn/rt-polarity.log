Namespace(bert_model='bert-base-uncased', data_dir='datasets', do_lower_case=False, gpu=3, learning_rate=5e-05, max_seq_length=64, num_train_epochs=9.0, output_dir='aug_data_1_4_3_0.5', sample_num=1, sample_ratio=4, seed=42, task_name='rt-polarity', temp=0.5, train_batch_size=32, warmup_proportion=0.1)
10/26/2019 13:17:33 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.
10/26/2019 13:17:35 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-1
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] . . . one of the most ing ##eni ##ous and entertaining thriller ##s i ' ve seen in quite a long time . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 1012 1012 1012 2028 1997 1996 2087 13749 18595 3560 1998 14036 10874 2015 1045 1005 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 1012 1012 1012 103 1997 1996 2087 13749 18595 3560 1998 14036 103 2015 103 103 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 2028 -1 -1 -1 -1 -1 -1 -1 -1 10874 -1 1045 1005 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-2
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] patch ##y combination of soap opera , low - tech magic realism and , at times , pl ##od ##ding ##ly sociological commentary . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 8983 2100 5257 1997 7815 3850 1010 2659 1011 6627 3894 15650 1998 1010 2012 2335 1010 20228 7716 4667 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 8983 2100 5257 1997 7815 3850 1010 103 1011 6627 3894 103 1998 1010 2012 2335 1010 20228 103 103 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 -1 -1 -1 -1 2659 -1 -1 -1 15650 -1 -1 -1 -1 -1 -1 7716 4667 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:40 - INFO - __main__ -   ***** Running training *****
10/26/2019 13:17:40 - INFO - __main__ -     Num examples = 8635
10/26/2019 13:17:40 - INFO - __main__ -     Batch size = 32
10/26/2019 13:17:40 - INFO - __main__ -     Num steps = 2428
constract vocabulary based on frequency
# train data: 8635
# test  data: 960
# vocab: 20399
# class: 2
before augment best acc:0.749766
Epoch:   0%|          | 0/9 [00:00<?, ?it/s]constract vocabulary based on frequency
# train data: 17227
# test  data: 960
# vocab: 23585
# class: 2
epoch 0 augment best acc:0.770384
Epoch:  11%|█         | 1/9 [14:45<1:58:02, 885.30s/it]constract vocabulary based on frequency
# train data: 17263
# test  data: 960
# vocab: 23171
# class: 2
epoch 1 augment best acc:0.748828
Epoch:  22%|██▏       | 2/9 [28:47<1:41:47, 872.49s/it]constract vocabulary based on frequency
# train data: 17272
# test  data: 960
# vocab: 23229
# class: 2
epoch 2 augment best acc:0.763824
Epoch:  33%|███▎      | 3/9 [38:59<1:19:24, 794.15s/it]constract vocabulary based on frequency
# train data: 17264
# test  data: 960
# vocab: 23129
# class: 2
epoch 3 augment best acc:0.761949
Epoch:  44%|████▍     | 4/9 [48:50<1:01:06, 733.26s/it]constract vocabulary based on frequency
# train data: 17272
# test  data: 960
# vocab: 23207
# class: 2
epoch 4 augment best acc:0.755389
Epoch:  56%|█████▌    | 5/9 [1:01:46<49:44, 746.25s/it]constract vocabulary based on frequency
# train data: 17267
# test  data: 960
# vocab: 23200
# class: 2
epoch 5 augment best acc:0.757263
Epoch:  67%|██████▋   | 6/9 [1:15:08<38:08, 762.89s/it]constract vocabulary based on frequency
# train data: 17269
# test  data: 960
# vocab: 23090
# class: 2
epoch 6 augment best acc:0.76851
Epoch:  78%|███████▊  | 7/9 [1:26:42<24:44, 742.07s/it]constract vocabulary based on frequency
# train data: 17270
# test  data: 960
# vocab: 23085
# class: 2
epoch 7 augment best acc:0.762887
Epoch:  89%|████████▉ | 8/9 [1:37:01<11:45, 705.37s/it]constract vocabulary based on frequency
# train data: 17275
# test  data: 960
# vocab: 23197
# class: 2
epoch 8 augment best acc:0.77507
Epoch: 100%|██████████| 9/9 [1:48:49<00:00, 706.11s/it]Epoch: 100%|██████████| 9/9 [1:48:49<00:00, 725.53s/it]
Namespace(bert_model='bert-base-uncased', data_dir='datasets', do_lower_case=False, gpu=3, learning_rate=5e-05, max_seq_length=64, num_train_epochs=9.0, output_dir='aug_data_1_4_3_1.0', sample_num=1, sample_ratio=4, seed=42, task_name='rt-polarity', temp=1.0, train_batch_size=32, warmup_proportion=0.1)
10/26/2019 13:17:33 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.
10/26/2019 13:17:35 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-1
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] . . . one of the most ing ##eni ##ous and entertaining thriller ##s i ' ve seen in quite a long time . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 1012 1012 1012 2028 1997 1996 2087 13749 18595 3560 1998 14036 10874 2015 1045 1005 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 1012 1012 1012 103 1997 1996 2087 13749 18595 3560 1998 14036 103 2015 103 103 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 2028 -1 -1 -1 -1 -1 -1 -1 -1 10874 -1 1045 1005 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-2
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] patch ##y combination of soap opera , low - tech magic realism and , at times , pl ##od ##ding ##ly sociological commentary . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 8983 2100 5257 1997 7815 3850 1010 2659 1011 6627 3894 15650 1998 1010 2012 2335 1010 20228 7716 4667 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 8983 2100 5257 1997 7815 3850 1010 103 1011 6627 3894 103 1998 1010 2012 2335 1010 20228 103 103 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 -1 -1 -1 -1 2659 -1 -1 -1 15650 -1 -1 -1 -1 -1 -1 7716 4667 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:40 - INFO - __main__ -   ***** Running training *****
10/26/2019 13:17:40 - INFO - __main__ -     Num examples = 8635
10/26/2019 13:17:40 - INFO - __main__ -     Batch size = 32
10/26/2019 13:17:40 - INFO - __main__ -     Num steps = 2428
constract vocabulary based on frequency
# train data: 8635
# test  data: 960
# vocab: 20399
# class: 2
before augment best acc:0.762887
Epoch:   0%|          | 0/9 [00:00<?, ?it/s]constract vocabulary based on frequency
# train data: 17214
# test  data: 960
# vocab: 25071
# class: 2
epoch 0 augment best acc:0.777882
Epoch:  11%|█         | 1/9 [13:50<1:50:46, 830.83s/it]constract vocabulary based on frequency
# train data: 17316
# test  data: 960
# vocab: 23973
# class: 2
epoch 1 augment best acc:0.755389
Epoch:  22%|██▏       | 2/9 [19:40<1:20:05, 686.57s/it]constract vocabulary based on frequency
# train data: 17234
# test  data: 960
# vocab: 23923
# class: 2
epoch 2 augment best acc:0.758201
Epoch:  33%|███▎      | 3/9 [32:23<1:10:55, 709.27s/it]constract vocabulary based on frequency
# train data: 17277
# test  data: 960
# vocab: 23822
# class: 2
epoch 3 augment best acc:0.74508
Epoch:  44%|████▍     | 4/9 [45:58<1:01:45, 741.10s/it]constract vocabulary based on frequency
# train data: 17271
# test  data: 960
# vocab: 23814
# class: 2
epoch 4 augment best acc:0.761949
Epoch:  56%|█████▌    | 5/9 [57:28<48:22, 725.68s/it]  constract vocabulary based on frequency
# train data: 17296
# test  data: 960
# vocab: 23804
# class: 2
epoch 5 augment best acc:0.761949
Epoch:  67%|██████▋   | 6/9 [1:05:40<32:47, 655.75s/it]constract vocabulary based on frequency
# train data: 17260
# test  data: 960
# vocab: 23647
# class: 2
epoch 6 augment best acc:0.764761
Epoch:  78%|███████▊  | 7/9 [1:18:02<22:42, 681.46s/it]constract vocabulary based on frequency
# train data: 17276
# test  data: 960
# vocab: 23636
# class: 2
epoch 7 augment best acc:0.765698
Epoch:  89%|████████▉ | 8/9 [1:30:39<11:44, 704.27s/it]constract vocabulary based on frequency
# train data: 17284
# test  data: 960
# vocab: 23653
# class: 2
epoch 8 augment best acc:0.773196
Epoch: 100%|██████████| 9/9 [1:40:54<00:00, 677.40s/it]Epoch: 100%|██████████| 9/9 [1:40:54<00:00, 672.70s/it]
Namespace(bert_model='bert-base-uncased', data_dir='datasets', do_lower_case=False, gpu=2, learning_rate=5e-05, max_seq_length=64, num_train_epochs=9.0, output_dir='aug_data_1_6_2_0.5', sample_num=1, sample_ratio=6, seed=42, task_name='rt-polarity', temp=0.5, train_batch_size=32, warmup_proportion=0.1)
10/26/2019 13:17:33 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.
10/26/2019 13:17:35 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-1
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] . . . one of the most ing ##eni ##ous and entertaining thriller ##s i ' ve seen in quite a long time . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 1012 1012 1012 2028 1997 1996 2087 13749 18595 3560 1998 14036 10874 2015 1045 1005 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 1012 1012 1012 103 1997 1996 2087 13749 18595 3560 1998 14036 103 2015 103 103 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 2028 -1 -1 -1 -1 -1 -1 -1 -1 10874 -1 1045 1005 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-2
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] patch ##y combination of soap opera , low - tech magic realism and , at times , pl ##od ##ding ##ly sociological commentary . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 8983 2100 5257 1997 7815 3850 1010 2659 1011 6627 3894 15650 1998 1010 2012 2335 1010 20228 7716 4667 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 8983 2100 5257 1997 7815 3850 1010 103 1011 6627 3894 103 1998 1010 2012 2335 1010 20228 103 103 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 -1 -1 -1 -1 2659 -1 -1 -1 15650 -1 -1 -1 -1 -1 -1 7716 4667 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:40 - INFO - __main__ -   ***** Running training *****
10/26/2019 13:17:40 - INFO - __main__ -     Num examples = 8635
10/26/2019 13:17:40 - INFO - __main__ -     Batch size = 32
10/26/2019 13:17:40 - INFO - __main__ -     Num steps = 2428
constract vocabulary based on frequency
# train data: 8635
# test  data: 960
# vocab: 20399
# class: 2
before augment best acc:0.746954
Epoch:   0%|          | 0/9 [00:00<?, ?it/s]constract vocabulary based on frequency
# train data: 17221
# test  data: 960
# vocab: 22624
# class: 2
epoch 0 augment best acc:0.759138
Epoch:  11%|█         | 1/9 [14:46<1:58:14, 886.82s/it]constract vocabulary based on frequency
# train data: 17275
# test  data: 960
# vocab: 22427
# class: 2
epoch 1 augment best acc:0.754452
Epoch:  22%|██▏       | 2/9 [28:22<1:40:58, 865.49s/it]constract vocabulary based on frequency
# train data: 17290
# test  data: 960
# vocab: 22512
# class: 2
epoch 2 augment best acc:0.772259
Epoch:  33%|███▎      | 3/9 [36:20<1:14:55, 749.24s/it]constract vocabulary based on frequency
# train data: 17273
# test  data: 960
# vocab: 22473
# class: 2
epoch 3 augment best acc:0.764761
Epoch:  44%|████▍     | 4/9 [47:48<1:00:54, 730.90s/it]constract vocabulary based on frequency
# train data: 17277
# test  data: 960
# vocab: 22453
# class: 2
epoch 4 augment best acc:0.764761
Epoch:  56%|█████▌    | 5/9 [1:01:28<50:30, 757.73s/it]constract vocabulary based on frequency
# train data: 17278
# test  data: 960
# vocab: 22479
# class: 2
epoch 5 augment best acc:0.765698
Epoch:  67%|██████▋   | 6/9 [1:14:21<38:06, 762.28s/it]constract vocabulary based on frequency
# train data: 17263
# test  data: 960
# vocab: 22397
# class: 2
epoch 6 augment best acc:0.764761
Epoch:  78%|███████▊  | 7/9 [1:25:17<24:20, 730.18s/it]constract vocabulary based on frequency
# train data: 17266
# test  data: 960
# vocab: 22404
# class: 2
epoch 7 augment best acc:0.766635
Epoch:  89%|████████▉ | 8/9 [1:36:17<11:49, 709.28s/it]constract vocabulary based on frequency
# train data: 17275
# test  data: 960
# vocab: 22406
# class: 2
epoch 8 augment best acc:0.758201
Epoch: 100%|██████████| 9/9 [1:48:33<00:00, 717.11s/it]Epoch: 100%|██████████| 9/9 [1:48:33<00:00, 723.67s/it]
Namespace(bert_model='bert-base-uncased', data_dir='datasets', do_lower_case=False, gpu=2, learning_rate=5e-05, max_seq_length=64, num_train_epochs=9.0, output_dir='aug_data_1_6_2_1.0', sample_num=1, sample_ratio=6, seed=42, task_name='rt-polarity', temp=1.0, train_batch_size=32, warmup_proportion=0.1)
10/26/2019 13:17:33 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.
10/26/2019 13:17:35 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-1
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] . . . one of the most ing ##eni ##ous and entertaining thriller ##s i ' ve seen in quite a long time . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 1012 1012 1012 2028 1997 1996 2087 13749 18595 3560 1998 14036 10874 2015 1045 1005 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 1012 1012 1012 103 1997 1996 2087 13749 18595 3560 1998 14036 103 2015 103 103 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 2028 -1 -1 -1 -1 -1 -1 -1 -1 10874 -1 1045 1005 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-2
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] patch ##y combination of soap opera , low - tech magic realism and , at times , pl ##od ##ding ##ly sociological commentary . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 8983 2100 5257 1997 7815 3850 1010 2659 1011 6627 3894 15650 1998 1010 2012 2335 1010 20228 7716 4667 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 8983 2100 5257 1997 7815 3850 1010 103 1011 6627 3894 103 1998 1010 2012 2335 1010 20228 103 103 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 -1 -1 -1 -1 2659 -1 -1 -1 15650 -1 -1 -1 -1 -1 -1 7716 4667 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:40 - INFO - __main__ -   ***** Running training *****
10/26/2019 13:17:40 - INFO - __main__ -     Num examples = 8635
10/26/2019 13:17:40 - INFO - __main__ -     Batch size = 32
10/26/2019 13:17:40 - INFO - __main__ -     Num steps = 2428
constract vocabulary based on frequency
# train data: 8635
# test  data: 960
# vocab: 20399
# class: 2
before augment best acc:0.752577
Epoch:   0%|          | 0/9 [00:00<?, ?it/s]constract vocabulary based on frequency
# train data: 17218
# test  data: 960
# vocab: 23434
# class: 2
epoch 0 augment best acc:0.766635
Epoch:  11%|█         | 1/9 [14:48<1:58:27, 888.41s/it]constract vocabulary based on frequency
# train data: 17300
# test  data: 960
# vocab: 22847
# class: 2
epoch 1 augment best acc:0.762887
Epoch:  22%|██▏       | 2/9 [29:05<1:42:33, 879.00s/it]constract vocabulary based on frequency
# train data: 17248
# test  data: 960
# vocab: 22896
# class: 2
epoch 2 augment best acc:0.778819
Epoch:  33%|███▎      | 3/9 [40:34<1:22:11, 821.93s/it]constract vocabulary based on frequency
# train data: 17276
# test  data: 960
# vocab: 22816
# class: 2
epoch 3 augment best acc:0.764761
Epoch:  44%|████▍     | 4/9 [49:14<1:00:57, 731.56s/it]constract vocabulary based on frequency
# train data: 17265
# test  data: 960
# vocab: 22772
# class: 2
epoch 4 augment best acc:0.774133
Epoch:  56%|█████▌    | 5/9 [1:01:29<48:50, 732.55s/it]constract vocabulary based on frequency
# train data: 17274
# test  data: 960
# vocab: 22774
# class: 2
epoch 5 augment best acc:0.767573
Epoch:  67%|██████▋   | 6/9 [1:14:32<37:23, 747.67s/it]constract vocabulary based on frequency
# train data: 17271
# test  data: 960
# vocab: 22718
# class: 2
epoch 6 augment best acc:0.777882
Epoch:  78%|███████▊  | 7/9 [1:25:48<24:12, 726.14s/it]constract vocabulary based on frequency
# train data: 17277
# test  data: 960
# vocab: 22728
# class: 2
epoch 7 augment best acc:0.766635
Epoch:  89%|████████▉ | 8/9 [1:36:41<11:44, 704.22s/it]constract vocabulary based on frequency
# train data: 17268
# test  data: 960
# vocab: 22728
# class: 2
epoch 8 augment best acc:0.761012
Epoch: 100%|██████████| 9/9 [1:48:50<00:00, 711.52s/it]Epoch: 100%|██████████| 9/9 [1:48:50<00:00, 725.58s/it]
Namespace(bert_model='bert-base-uncased', data_dir='datasets', do_lower_case=False, gpu=5, learning_rate=5e-05, max_seq_length=64, num_train_epochs=9.0, output_dir='aug_data_2_4_5_0.5', sample_num=2, sample_ratio=4, seed=42, task_name='rt-polarity', temp=0.5, train_batch_size=32, warmup_proportion=0.1)
10/26/2019 13:17:33 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.
10/26/2019 13:17:35 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-1
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] . . . one of the most ing ##eni ##ous and entertaining thriller ##s i ' ve seen in quite a long time . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 1012 1012 1012 2028 1997 1996 2087 13749 18595 3560 1998 14036 10874 2015 1045 1005 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 1012 1012 1012 103 1997 1996 2087 13749 18595 3560 1998 14036 103 2015 103 103 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 2028 -1 -1 -1 -1 -1 -1 -1 -1 10874 -1 1045 1005 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-2
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] patch ##y combination of soap opera , low - tech magic realism and , at times , pl ##od ##ding ##ly sociological commentary . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 8983 2100 5257 1997 7815 3850 1010 2659 1011 6627 3894 15650 1998 1010 2012 2335 1010 20228 7716 4667 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 8983 2100 5257 1997 7815 3850 1010 103 1011 6627 3894 103 1998 1010 2012 2335 1010 20228 103 103 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 -1 -1 -1 -1 2659 -1 -1 -1 15650 -1 -1 -1 -1 -1 -1 7716 4667 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:40 - INFO - __main__ -   ***** Running training *****
10/26/2019 13:17:40 - INFO - __main__ -     Num examples = 8635
10/26/2019 13:17:40 - INFO - __main__ -     Batch size = 32
10/26/2019 13:17:40 - INFO - __main__ -     Num steps = 2428
constract vocabulary based on frequency
# train data: 8635
# test  data: 960
# vocab: 20399
# class: 2
before augment best acc:0.761012
Epoch:   0%|          | 0/9 [00:00<?, ?it/s]constract vocabulary based on frequency
# train data: 25832
# test  data: 960
# vocab: 24782
# class: 2
epoch 0 augment best acc:0.750703
Epoch:  11%|█         | 1/9 [15:52<2:06:56, 952.11s/it]constract vocabulary based on frequency
# train data: 25936
# test  data: 960
# vocab: 24250
# class: 2
epoch 1 augment best acc:0.761949
Epoch:  22%|██▏       | 2/9 [30:58<1:49:29, 938.45s/it]constract vocabulary based on frequency
# train data: 25925
# test  data: 960
# vocab: 24339
# class: 2
epoch 2 augment best acc:0.757263
Epoch:  33%|███▎      | 3/9 [45:40<1:32:08, 921.50s/it]constract vocabulary based on frequency
# train data: 25875
# test  data: 960
# vocab: 24201
# class: 2
epoch 3 augment best acc:0.759138
Epoch:  44%|████▍     | 4/9 [59:09<1:13:59, 887.85s/it]constract vocabulary based on frequency
# train data: 25908
# test  data: 960
# vocab: 24100
# class: 2
epoch 4 augment best acc:0.749766
Epoch:  56%|█████▌    | 5/9 [1:09:47<54:11, 812.88s/it]constract vocabulary based on frequency
# train data: 25923
# test  data: 960
# vocab: 24162
# class: 2
epoch 5 augment best acc:0.769447
Epoch:  67%|██████▋   | 6/9 [1:21:09<38:40, 773.36s/it]constract vocabulary based on frequency
# train data: 25883
# test  data: 960
# vocab: 23960
# class: 2
epoch 6 augment best acc:0.764761
Epoch:  78%|███████▊  | 7/9 [1:34:32<26:05, 782.51s/it]constract vocabulary based on frequency
# train data: 25924
# test  data: 960
# vocab: 24053
# class: 2
epoch 7 augment best acc:0.757263
Epoch:  89%|████████▉ | 8/9 [1:48:06<13:11, 791.89s/it]constract vocabulary based on frequency
# train data: 25903
# test  data: 960
# vocab: 23995
# class: 2
epoch 8 augment best acc:0.772259
Epoch: 100%|██████████| 9/9 [1:58:00<00:00, 732.39s/it]Epoch: 100%|██████████| 9/9 [1:58:00<00:00, 786.69s/it]
Namespace(bert_model='bert-base-uncased', data_dir='datasets', do_lower_case=False, gpu=5, learning_rate=5e-05, max_seq_length=64, num_train_epochs=9.0, output_dir='aug_data_2_4_5_1.0', sample_num=2, sample_ratio=4, seed=42, task_name='rt-polarity', temp=1.0, train_batch_size=32, warmup_proportion=0.1)
10/26/2019 13:17:33 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.
10/26/2019 13:17:35 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-1
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] . . . one of the most ing ##eni ##ous and entertaining thriller ##s i ' ve seen in quite a long time . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 1012 1012 1012 2028 1997 1996 2087 13749 18595 3560 1998 14036 10874 2015 1045 1005 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 1012 1012 1012 103 1997 1996 2087 13749 18595 3560 1998 14036 103 2015 103 103 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 2028 -1 -1 -1 -1 -1 -1 -1 -1 10874 -1 1045 1005 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-2
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] patch ##y combination of soap opera , low - tech magic realism and , at times , pl ##od ##ding ##ly sociological commentary . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 8983 2100 5257 1997 7815 3850 1010 2659 1011 6627 3894 15650 1998 1010 2012 2335 1010 20228 7716 4667 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 8983 2100 5257 1997 7815 3850 1010 103 1011 6627 3894 103 1998 1010 2012 2335 1010 20228 103 103 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 -1 -1 -1 -1 2659 -1 -1 -1 15650 -1 -1 -1 -1 -1 -1 7716 4667 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:40 - INFO - __main__ -   ***** Running training *****
10/26/2019 13:17:40 - INFO - __main__ -     Num examples = 8635
10/26/2019 13:17:40 - INFO - __main__ -     Batch size = 32
10/26/2019 13:17:40 - INFO - __main__ -     Num steps = 2428
constract vocabulary based on frequency
# train data: 8635
# test  data: 960
# vocab: 20399
# class: 2
before augment best acc:0.769447
Epoch:   0%|          | 0/9 [00:00<?, ?it/s]constract vocabulary based on frequency
# train data: 25865
# test  data: 960
# vocab: 28385
# class: 2
epoch 0 augment best acc:0.763824
Epoch:  11%|█         | 1/9 [15:34<2:04:35, 934.42s/it]constract vocabulary based on frequency
# train data: 25923
# test  data: 960
# vocab: 26263
# class: 2
epoch 1 augment best acc:0.774133
Epoch:  22%|██▏       | 2/9 [30:18<1:47:15, 919.29s/it]constract vocabulary based on frequency
# train data: 25904
# test  data: 960
# vocab: 26041
# class: 2
epoch 2 augment best acc:0.766635
Epoch:  33%|███▎      | 3/9 [43:33<1:28:11, 881.94s/it]constract vocabulary based on frequency
# train data: 25866
# test  data: 960
# vocab: 25870
# class: 2
epoch 3 augment best acc:0.761949
Epoch:  44%|████▍     | 4/9 [54:22<1:07:40, 812.17s/it]constract vocabulary based on frequency
# train data: 25923
# test  data: 960
# vocab: 25804
# class: 2
epoch 4 augment best acc:0.762887
Epoch:  56%|█████▌    | 5/9 [1:05:31<51:17, 769.32s/it]constract vocabulary based on frequency
# train data: 25907
# test  data: 960
# vocab: 25817
# class: 2
epoch 5 augment best acc:0.76851
Epoch:  67%|██████▋   | 6/9 [1:19:06<39:08, 782.89s/it]constract vocabulary based on frequency
# train data: 25907
# test  data: 960
# vocab: 25456
# class: 2
epoch 6 augment best acc:0.762887
Epoch:  78%|███████▊  | 7/9 [1:33:21<26:48, 804.40s/it]constract vocabulary based on frequency
# train data: 25939
# test  data: 960
# vocab: 25469
# class: 2
epoch 7 augment best acc:0.750703
Epoch:  89%|████████▉ | 8/9 [1:46:14<13:15, 795.22s/it]constract vocabulary based on frequency
# train data: 25897
# test  data: 960
# vocab: 25432
# class: 2
epoch 8 augment best acc:0.758201
Epoch: 100%|██████████| 9/9 [1:53:25<00:00, 685.92s/it]Epoch: 100%|██████████| 9/9 [1:53:25<00:00, 756.19s/it]
Namespace(bert_model='bert-base-uncased', data_dir='datasets', do_lower_case=False, gpu=4, learning_rate=5e-05, max_seq_length=64, num_train_epochs=9.0, output_dir='aug_data_2_6_4_0.5', sample_num=2, sample_ratio=6, seed=42, task_name='rt-polarity', temp=0.5, train_batch_size=32, warmup_proportion=0.1)
10/26/2019 13:17:34 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.
10/26/2019 13:17:35 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-1
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] . . . one of the most ing ##eni ##ous and entertaining thriller ##s i ' ve seen in quite a long time . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 1012 1012 1012 2028 1997 1996 2087 13749 18595 3560 1998 14036 10874 2015 1045 1005 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 1012 1012 1012 103 1997 1996 2087 13749 18595 3560 1998 14036 103 2015 103 103 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 2028 -1 -1 -1 -1 -1 -1 -1 -1 10874 -1 1045 1005 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-2
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] patch ##y combination of soap opera , low - tech magic realism and , at times , pl ##od ##ding ##ly sociological commentary . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 8983 2100 5257 1997 7815 3850 1010 2659 1011 6627 3894 15650 1998 1010 2012 2335 1010 20228 7716 4667 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 8983 2100 5257 1997 7815 3850 1010 103 1011 6627 3894 103 1998 1010 2012 2335 1010 20228 103 103 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 -1 -1 -1 -1 2659 -1 -1 -1 15650 -1 -1 -1 -1 -1 -1 7716 4667 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:40 - INFO - __main__ -   ***** Running training *****
10/26/2019 13:17:40 - INFO - __main__ -     Num examples = 8635
10/26/2019 13:17:40 - INFO - __main__ -     Batch size = 32
10/26/2019 13:17:40 - INFO - __main__ -     Num steps = 2428
constract vocabulary based on frequency
# train data: 8635
# test  data: 960
# vocab: 20399
# class: 2
before augment best acc:0.754452
Epoch:   0%|          | 0/9 [00:00<?, ?it/s]constract vocabulary based on frequency
# train data: 25878
# test  data: 960
# vocab: 23313
# class: 2
epoch 0 augment best acc:0.765698
Epoch:  11%|█         | 1/9 [15:31<2:04:11, 931.40s/it]constract vocabulary based on frequency
# train data: 25886
# test  data: 960
# vocab: 23071
# class: 2
epoch 1 augment best acc:0.781631
Epoch:  22%|██▏       | 2/9 [30:55<1:48:25, 929.29s/it]constract vocabulary based on frequency
# train data: 25933
# test  data: 960
# vocab: 23023
# class: 2
epoch 2 augment best acc:0.766635
Epoch:  33%|███▎      | 3/9 [45:54<1:32:00, 920.03s/it]constract vocabulary based on frequency
# train data: 25880
# test  data: 960
# vocab: 22972
# class: 2
epoch 3 augment best acc:0.755389
Epoch:  44%|████▍     | 4/9 [59:32<1:14:07, 889.40s/it]constract vocabulary based on frequency
# train data: 25907
# test  data: 960
# vocab: 22858
# class: 2
epoch 4 augment best acc:0.767573
Epoch:  56%|█████▌    | 5/9 [1:11:59<56:27, 846.88s/it]constract vocabulary based on frequency
# train data: 25926
# test  data: 960
# vocab: 22923
# class: 2
epoch 5 augment best acc:0.758201
Epoch:  67%|██████▋   | 6/9 [1:23:08<39:40, 793.43s/it]constract vocabulary based on frequency
# train data: 25890
# test  data: 960
# vocab: 22766
# class: 2
epoch 6 augment best acc:0.755389
Epoch:  78%|███████▊  | 7/9 [1:35:45<26:05, 782.50s/it]constract vocabulary based on frequency
# train data: 25911
# test  data: 960
# vocab: 22815
# class: 2
epoch 7 augment best acc:0.756326
Epoch:  89%|████████▉ | 8/9 [1:49:27<13:14, 794.40s/it]constract vocabulary based on frequency
# train data: 25903
# test  data: 960
# vocab: 22873
# class: 2
epoch 8 augment best acc:0.771321
Epoch: 100%|██████████| 9/9 [1:59:44<00:00, 741.02s/it]Epoch: 100%|██████████| 9/9 [1:59:44<00:00, 798.24s/it]
Namespace(bert_model='bert-base-uncased', data_dir='datasets', do_lower_case=False, gpu=4, learning_rate=5e-05, max_seq_length=64, num_train_epochs=9.0, output_dir='aug_data_2_6_4_1.0', sample_num=2, sample_ratio=6, seed=42, task_name='rt-polarity', temp=1.0, train_batch_size=32, warmup_proportion=0.1)
10/26/2019 13:17:33 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.
10/26/2019 13:17:35 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-1
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] . . . one of the most ing ##eni ##ous and entertaining thriller ##s i ' ve seen in quite a long time . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 1012 1012 1012 2028 1997 1996 2087 13749 18595 3560 1998 14036 10874 2015 1045 1005 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 1012 1012 1012 103 1997 1996 2087 13749 18595 3560 1998 14036 103 2015 103 103 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 2028 -1 -1 -1 -1 -1 -1 -1 -1 10874 -1 1045 1005 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-2
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] patch ##y combination of soap opera , low - tech magic realism and , at times , pl ##od ##ding ##ly sociological commentary . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 8983 2100 5257 1997 7815 3850 1010 2659 1011 6627 3894 15650 1998 1010 2012 2335 1010 20228 7716 4667 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 8983 2100 5257 1997 7815 3850 1010 103 1011 6627 3894 103 1998 1010 2012 2335 1010 20228 103 103 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 -1 -1 -1 -1 2659 -1 -1 -1 15650 -1 -1 -1 -1 -1 -1 7716 4667 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:40 - INFO - __main__ -   ***** Running training *****
10/26/2019 13:17:40 - INFO - __main__ -     Num examples = 8635
10/26/2019 13:17:40 - INFO - __main__ -     Batch size = 32
10/26/2019 13:17:40 - INFO - __main__ -     Num steps = 2428
constract vocabulary based on frequency
# train data: 8635
# test  data: 960
# vocab: 20399
# class: 2
before augment best acc:0.75164
Epoch:   0%|          | 0/9 [00:00<?, ?it/s]constract vocabulary based on frequency
# train data: 25891
# test  data: 960
# vocab: 25271
# class: 2
epoch 0 augment best acc:0.765698
Epoch:  11%|█         | 1/9 [15:55<2:07:21, 955.16s/it]constract vocabulary based on frequency
# train data: 25847
# test  data: 960
# vocab: 23991
# class: 2
epoch 1 augment best acc:0.765698
Epoch:  22%|██▏       | 2/9 [31:22<1:50:27, 946.74s/it]constract vocabulary based on frequency
# train data: 25968
# test  data: 960
# vocab: 23852
# class: 2
epoch 2 augment best acc:0.772259
Epoch:  33%|███▎      | 3/9 [46:28<1:33:26, 934.47s/it]constract vocabulary based on frequency
# train data: 25900
# test  data: 960
# vocab: 23888
# class: 2
epoch 3 augment best acc:0.764761
Epoch:  44%|████▍     | 4/9 [1:00:41<1:15:51, 910.29s/it]constract vocabulary based on frequency
# train data: 25915
# test  data: 960
# vocab: 23719
# class: 2
epoch 4 augment best acc:0.759138
Epoch:  56%|█████▌    | 5/9 [1:14:37<59:11, 887.96s/it]  constract vocabulary based on frequency
# train data: 25907
# test  data: 960
# vocab: 23786
# class: 2
epoch 5 augment best acc:0.759138
Epoch:  67%|██████▋   | 6/9 [1:27:19<42:30, 850.18s/it]constract vocabulary based on frequency
# train data: 25906
# test  data: 960
# vocab: 23624
# class: 2
epoch 6 augment best acc:0.753515
Epoch:  78%|███████▊  | 7/9 [1:39:10<26:56, 808.42s/it]constract vocabulary based on frequency
# train data: 25856
# test  data: 960
# vocab: 23587
# class: 2
epoch 7 augment best acc:0.765698
Epoch:  89%|████████▉ | 8/9 [1:50:39<12:52, 772.58s/it]constract vocabulary based on frequency
# train data: 25907
# test  data: 960
# vocab: 23618
# class: 2
epoch 8 augment best acc:0.761949
Epoch: 100%|██████████| 9/9 [2:00:03<00:00, 709.96s/it]Epoch: 100%|██████████| 9/9 [2:00:03<00:00, 800.40s/it]
Namespace(bert_model='bert-base-uncased', data_dir='datasets', do_lower_case=False, gpu=7, learning_rate=5e-05, max_seq_length=64, num_train_epochs=9.0, output_dir='aug_data_3_4_7_0.5', sample_num=3, sample_ratio=4, seed=42, task_name='rt-polarity', temp=0.5, train_batch_size=32, warmup_proportion=0.1)
10/26/2019 13:17:34 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.
10/26/2019 13:17:35 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-1
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] . . . one of the most ing ##eni ##ous and entertaining thriller ##s i ' ve seen in quite a long time . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 1012 1012 1012 2028 1997 1996 2087 13749 18595 3560 1998 14036 10874 2015 1045 1005 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 1012 1012 1012 103 1997 1996 2087 13749 18595 3560 1998 14036 103 2015 103 103 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 2028 -1 -1 -1 -1 -1 -1 -1 -1 10874 -1 1045 1005 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-2
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] patch ##y combination of soap opera , low - tech magic realism and , at times , pl ##od ##ding ##ly sociological commentary . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 8983 2100 5257 1997 7815 3850 1010 2659 1011 6627 3894 15650 1998 1010 2012 2335 1010 20228 7716 4667 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 8983 2100 5257 1997 7815 3850 1010 103 1011 6627 3894 103 1998 1010 2012 2335 1010 20228 103 103 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 -1 -1 -1 -1 2659 -1 -1 -1 15650 -1 -1 -1 -1 -1 -1 7716 4667 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:40 - INFO - __main__ -   ***** Running training *****
10/26/2019 13:17:40 - INFO - __main__ -     Num examples = 8635
10/26/2019 13:17:40 - INFO - __main__ -     Batch size = 32
10/26/2019 13:17:40 - INFO - __main__ -     Num steps = 2428
constract vocabulary based on frequency
# train data: 8635
# test  data: 960
# vocab: 20399
# class: 2
before augment best acc:0.757263
Epoch:   0%|          | 0/9 [00:00<?, ?it/s]constract vocabulary based on frequency
# train data: 34540
# test  data: 960
# vocab: 25808
# class: 2
epoch 0 augment best acc:0.744142
Epoch:  11%|█         | 1/9 [16:56<2:15:35, 1016.99s/it]constract vocabulary based on frequency
# train data: 34535
# test  data: 960
# vocab: 25047
# class: 2
epoch 1 augment best acc:0.757263
Epoch:  22%|██▏       | 2/9 [32:18<1:55:19, 988.46s/it] constract vocabulary based on frequency
# train data: 34500
# test  data: 960
# vocab: 25111
# class: 2
epoch 2 augment best acc:0.765698
Epoch:  33%|███▎      | 3/9 [47:51<1:37:11, 971.84s/it]constract vocabulary based on frequency
# train data: 34553
# test  data: 960
# vocab: 24968
# class: 2
epoch 3 augment best acc:0.757263
Epoch:  44%|████▍     | 4/9 [1:03:02<1:19:28, 953.60s/it]constract vocabulary based on frequency
# train data: 34519
# test  data: 960
# vocab: 24937
# class: 2
epoch 4 augment best acc:0.743205
Epoch:  56%|█████▌    | 5/9 [1:18:29<1:03:02, 945.61s/it]constract vocabulary based on frequency
# train data: 34580
# test  data: 960
# vocab: 24907
# class: 2
epoch 5 augment best acc:0.761012
Epoch:  67%|██████▋   | 6/9 [1:33:56<47:00, 940.00s/it]  constract vocabulary based on frequency
# train data: 34529
# test  data: 960
# vocab: 24618
# class: 2
epoch 6 augment best acc:0.757263
Epoch:  78%|███████▊  | 7/9 [1:48:28<30:38, 919.38s/it]constract vocabulary based on frequency
# train data: 34557
# test  data: 960
# vocab: 24592
# class: 2
epoch 7 augment best acc:0.758201
Epoch:  89%|████████▉ | 8/9 [2:00:27<14:19, 859.32s/it]constract vocabulary based on frequency
# train data: 34533
# test  data: 960
# vocab: 24793
# class: 2
epoch 8 augment best acc:0.75164
Epoch: 100%|██████████| 9/9 [2:09:35<00:00, 765.86s/it]Epoch: 100%|██████████| 9/9 [2:09:35<00:00, 863.90s/it]
Namespace(bert_model='bert-base-uncased', data_dir='datasets', do_lower_case=False, gpu=7, learning_rate=5e-05, max_seq_length=64, num_train_epochs=9.0, output_dir='aug_data_3_4_7_1.0', sample_num=3, sample_ratio=4, seed=42, task_name='rt-polarity', temp=1.0, train_batch_size=32, warmup_proportion=0.1)
10/26/2019 13:17:34 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.
10/26/2019 13:17:35 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-1
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] . . . one of the most ing ##eni ##ous and entertaining thriller ##s i ' ve seen in quite a long time . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 1012 1012 1012 2028 1997 1996 2087 13749 18595 3560 1998 14036 10874 2015 1045 1005 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 1012 1012 1012 103 1997 1996 2087 13749 18595 3560 1998 14036 103 2015 103 103 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 2028 -1 -1 -1 -1 -1 -1 -1 -1 10874 -1 1045 1005 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-2
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] patch ##y combination of soap opera , low - tech magic realism and , at times , pl ##od ##ding ##ly sociological commentary . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 8983 2100 5257 1997 7815 3850 1010 2659 1011 6627 3894 15650 1998 1010 2012 2335 1010 20228 7716 4667 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 8983 2100 5257 1997 7815 3850 1010 103 1011 6627 3894 103 1998 1010 2012 2335 1010 20228 103 103 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 -1 -1 -1 -1 2659 -1 -1 -1 15650 -1 -1 -1 -1 -1 -1 7716 4667 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:40 - INFO - __main__ -   ***** Running training *****
10/26/2019 13:17:40 - INFO - __main__ -     Num examples = 8635
10/26/2019 13:17:40 - INFO - __main__ -     Batch size = 32
10/26/2019 13:17:40 - INFO - __main__ -     Num steps = 2428
constract vocabulary based on frequency
# train data: 8635
# test  data: 960
# vocab: 20399
# class: 2
before augment best acc:0.766635
Epoch:   0%|          | 0/9 [00:00<?, ?it/s]constract vocabulary based on frequency
# train data: 34528
# test  data: 960
# vocab: 31509
# class: 2
epoch 0 augment best acc:0.772259
Epoch:  11%|█         | 1/9 [15:56<2:07:30, 956.26s/it]constract vocabulary based on frequency
# train data: 34516
# test  data: 960
# vocab: 28332
# class: 2
epoch 1 augment best acc:0.760075
Epoch:  22%|██▏       | 2/9 [32:07<1:52:06, 960.87s/it]constract vocabulary based on frequency
# train data: 34521
# test  data: 960
# vocab: 28083
# class: 2
epoch 2 augment best acc:0.752577
Epoch:  33%|███▎      | 3/9 [48:18<1:36:23, 963.92s/it]constract vocabulary based on frequency
# train data: 34555
# test  data: 960
# vocab: 27841
# class: 2
epoch 3 augment best acc:0.747891
Epoch:  44%|████▍     | 4/9 [1:04:05<1:19:53, 958.64s/it]constract vocabulary based on frequency
# train data: 34558
# test  data: 960
# vocab: 27511
# class: 2
epoch 4 augment best acc:0.759138
Epoch:  56%|█████▌    | 5/9 [1:19:09<1:02:49, 942.32s/it]constract vocabulary based on frequency
# train data: 34516
# test  data: 960
# vocab: 27484
# class: 2
epoch 5 augment best acc:0.761949
Epoch:  67%|██████▋   | 6/9 [1:34:27<46:44, 934.94s/it]  constract vocabulary based on frequency
# train data: 34515
# test  data: 960
# vocab: 27243
# class: 2
epoch 6 augment best acc:0.764761
Epoch:  78%|███████▊  | 7/9 [1:48:56<30:30, 915.38s/it]constract vocabulary based on frequency
# train data: 34575
# test  data: 960
# vocab: 27141
# class: 2
epoch 7 augment best acc:0.758201
Epoch:  89%|████████▉ | 8/9 [2:00:51<14:15, 855.05s/it]constract vocabulary based on frequency
# train data: 34543
# test  data: 960
# vocab: 27208
# class: 2
epoch 8 augment best acc:0.776007
Epoch: 100%|██████████| 9/9 [2:09:42<00:00, 757.83s/it]Epoch: 100%|██████████| 9/9 [2:09:42<00:00, 864.69s/it]
Namespace(bert_model='bert-base-uncased', data_dir='datasets', do_lower_case=False, gpu=6, learning_rate=5e-05, max_seq_length=64, num_train_epochs=9.0, output_dir='aug_data_3_6_6_0.5', sample_num=3, sample_ratio=6, seed=42, task_name='rt-polarity', temp=0.5, train_batch_size=32, warmup_proportion=0.1)
10/26/2019 13:17:33 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.
10/26/2019 13:17:35 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-1
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] . . . one of the most ing ##eni ##ous and entertaining thriller ##s i ' ve seen in quite a long time . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 1012 1012 1012 2028 1997 1996 2087 13749 18595 3560 1998 14036 10874 2015 1045 1005 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 1012 1012 1012 103 1997 1996 2087 13749 18595 3560 1998 14036 103 2015 103 103 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 2028 -1 -1 -1 -1 -1 -1 -1 -1 10874 -1 1045 1005 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-2
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] patch ##y combination of soap opera , low - tech magic realism and , at times , pl ##od ##ding ##ly sociological commentary . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 8983 2100 5257 1997 7815 3850 1010 2659 1011 6627 3894 15650 1998 1010 2012 2335 1010 20228 7716 4667 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 8983 2100 5257 1997 7815 3850 1010 103 1011 6627 3894 103 1998 1010 2012 2335 1010 20228 103 103 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 -1 -1 -1 -1 2659 -1 -1 -1 15650 -1 -1 -1 -1 -1 -1 7716 4667 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:40 - INFO - __main__ -   ***** Running training *****
10/26/2019 13:17:40 - INFO - __main__ -     Num examples = 8635
10/26/2019 13:17:40 - INFO - __main__ -     Batch size = 32
10/26/2019 13:17:40 - INFO - __main__ -     Num steps = 2428
constract vocabulary based on frequency
# train data: 8635
# test  data: 960
# vocab: 20399
# class: 2
before augment best acc:0.756326
Epoch:   0%|          | 0/9 [00:00<?, ?it/s]constract vocabulary based on frequency
# train data: 34474
# test  data: 960
# vocab: 24029
# class: 2
epoch 0 augment best acc:0.759138
Epoch:  11%|█         | 1/9 [16:32<2:12:21, 992.73s/it]constract vocabulary based on frequency
# train data: 34572
# test  data: 960
# vocab: 23377
# class: 2
epoch 1 augment best acc:0.764761
Epoch:  22%|██▏       | 2/9 [31:44<1:52:58, 968.32s/it]constract vocabulary based on frequency
# train data: 34540
# test  data: 960
# vocab: 23358
# class: 2
epoch 2 augment best acc:0.763824
Epoch:  33%|███▎      | 3/9 [47:23<1:35:57, 959.59s/it]constract vocabulary based on frequency
# train data: 34516
# test  data: 960
# vocab: 23306
# class: 2
epoch 3 augment best acc:0.761949
Epoch:  44%|████▍     | 4/9 [1:02:40<1:18:54, 946.98s/it]constract vocabulary based on frequency
# train data: 34554
# test  data: 960
# vocab: 23268
# class: 2
epoch 4 augment best acc:0.753515
Epoch:  56%|█████▌    | 5/9 [1:17:55<1:02:29, 937.38s/it]constract vocabulary based on frequency
# train data: 34530
# test  data: 960
# vocab: 23262
# class: 2
epoch 5 augment best acc:0.756326
Epoch:  67%|██████▋   | 6/9 [1:33:19<46:39, 933.16s/it]  constract vocabulary based on frequency
# train data: 34587
# test  data: 960
# vocab: 23143
# class: 2
epoch 6 augment best acc:0.765698
Epoch:  78%|███████▊  | 7/9 [1:48:05<30:38, 919.24s/it]constract vocabulary based on frequency
# train data: 34496
# test  data: 960
# vocab: 23220
# class: 2
epoch 7 augment best acc:0.761012
Epoch:  89%|████████▉ | 8/9 [1:58:53<13:57, 837.80s/it]constract vocabulary based on frequency
# train data: 34537
# test  data: 960
# vocab: 23107
# class: 2
epoch 8 augment best acc:0.749766
Epoch: 100%|██████████| 9/9 [2:07:27<00:00, 740.68s/it]Epoch: 100%|██████████| 9/9 [2:07:27<00:00, 849.75s/it]
Namespace(bert_model='bert-base-uncased', data_dir='datasets', do_lower_case=False, gpu=6, learning_rate=5e-05, max_seq_length=64, num_train_epochs=9.0, output_dir='aug_data_3_6_6_1.0', sample_num=3, sample_ratio=6, seed=42, task_name='rt-polarity', temp=1.0, train_batch_size=32, warmup_proportion=0.1)
10/26/2019 13:17:33 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.
10/26/2019 13:17:35 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-1
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] . . . one of the most ing ##eni ##ous and entertaining thriller ##s i ' ve seen in quite a long time . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 1012 1012 1012 2028 1997 1996 2087 13749 18595 3560 1998 14036 10874 2015 1045 1005 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 1012 1012 1012 103 1997 1996 2087 13749 18595 3560 1998 14036 103 2015 103 103 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 2028 -1 -1 -1 -1 -1 -1 -1 -1 10874 -1 1045 1005 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:35 - INFO - __main__ -   *** Example ***
10/26/2019 13:17:35 - INFO - __main__ -   guid: train-2
10/26/2019 13:17:35 - INFO - __main__ -   tokens: [CLS] patch ##y combination of soap opera , low - tech magic realism and , at times , pl ##od ##ding ##ly sociological commentary . [SEP]
10/26/2019 13:17:35 - INFO - __main__ -   init_ids: 101 8983 2100 5257 1997 7815 3850 1010 2659 1011 6627 3894 15650 1998 1010 2012 2335 1010 20228 7716 4667 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_ids: 101 8983 2100 5257 1997 7815 3850 1010 103 1011 6627 3894 103 1998 1010 2012 2335 1010 20228 103 103 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 13:17:35 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 -1 -1 -1 -1 2659 -1 -1 -1 15650 -1 -1 -1 -1 -1 -1 7716 4667 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 13:17:40 - INFO - __main__ -   ***** Running training *****
10/26/2019 13:17:40 - INFO - __main__ -     Num examples = 8635
10/26/2019 13:17:40 - INFO - __main__ -     Batch size = 32
10/26/2019 13:17:40 - INFO - __main__ -     Num steps = 2428
constract vocabulary based on frequency
# train data: 8635
# test  data: 960
# vocab: 20399
# class: 2
before augment best acc:0.763824
Epoch:   0%|          | 0/9 [00:00<?, ?it/s]constract vocabulary based on frequency
# train data: 34536
# test  data: 960
# vocab: 26910
# class: 2
epoch 0 augment best acc:0.752577
Epoch:  11%|█         | 1/9 [17:06<2:16:49, 1026.16s/it]constract vocabulary based on frequency
# train data: 34527
# test  data: 960
# vocab: 25053
# class: 2
epoch 1 augment best acc:0.747891
Epoch:  22%|██▏       | 2/9 [32:07<1:55:21, 988.81s/it] constract vocabulary based on frequency
# train data: 34512
# test  data: 960
# vocab: 24791
# class: 2
epoch 2 augment best acc:0.762887
Epoch:  33%|███▎      | 3/9 [47:45<1:37:20, 973.45s/it]constract vocabulary based on frequency
# train data: 34545
# test  data: 960
# vocab: 24648
# class: 2
epoch 3 augment best acc:0.744142
Epoch:  44%|████▍     | 4/9 [1:03:02<1:19:42, 956.56s/it]constract vocabulary based on frequency
# train data: 34525
# test  data: 960
# vocab: 24566
# class: 2
epoch 4 augment best acc:0.74508
Epoch:  56%|█████▌    | 5/9 [1:18:27<1:03:08, 947.09s/it]constract vocabulary based on frequency
# train data: 34581
# test  data: 960
# vocab: 24650
# class: 2
epoch 5 augment best acc:0.761949
Epoch:  67%|██████▋   | 6/9 [1:34:36<47:40, 953.53s/it]  constract vocabulary based on frequency
# train data: 34504
# test  data: 960
# vocab: 24374
# class: 2
epoch 6 augment best acc:0.757263
Epoch:  78%|███████▊  | 7/9 [1:49:54<31:25, 942.89s/it]constract vocabulary based on frequency
# train data: 34585
# test  data: 960
# vocab: 24519
# class: 2
epoch 7 augment best acc:0.76851
Epoch:  89%|████████▉ | 8/9 [2:00:46<14:15, 855.84s/it]constract vocabulary based on frequency
# train data: 34534
# test  data: 960
# vocab: 24468
# class: 2
epoch 8 augment best acc:0.762887
Epoch: 100%|██████████| 9/9 [2:10:01<00:00, 765.45s/it]Epoch: 100%|██████████| 9/9 [2:10:01<00:00, 866.83s/it]
