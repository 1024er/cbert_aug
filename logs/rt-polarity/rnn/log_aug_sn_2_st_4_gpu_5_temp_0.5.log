Namespace(bert_model='bert-base-uncased', data_dir='datasets', do_lower_case=False, gpu=5, learning_rate=5e-05, max_seq_length=64, num_train_epochs=9.0, output_dir='aug_data_2_4_5_0.5', sample_num=2, sample_ratio=4, seed=42, task_name='rt-polarity', temp=0.5, train_batch_size=32, warmup_proportion=0.1)
10/27/2019 03:39:49 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.
10/27/2019 03:39:51 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/27/2019 03:39:51 - INFO - __main__ -   *** Example ***
10/27/2019 03:39:51 - INFO - __main__ -   guid: train-1
10/27/2019 03:39:51 - INFO - __main__ -   tokens: [CLS] . . . one of the most ing ##eni ##ous and entertaining thriller ##s i ' ve seen in quite a long time . [SEP]
10/27/2019 03:39:51 - INFO - __main__ -   init_ids: 101 1012 1012 1012 2028 1997 1996 2087 13749 18595 3560 1998 14036 10874 2015 1045 1005 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/27/2019 03:39:51 - INFO - __main__ -   input_ids: 101 1012 1012 1012 103 1997 1996 2087 13749 18595 3560 1998 14036 103 2015 103 103 2310 2464 1999 3243 1037 2146 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/27/2019 03:39:51 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/27/2019 03:39:51 - INFO - __main__ -   segment_ids: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/27/2019 03:39:51 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 2028 -1 -1 -1 -1 -1 -1 -1 -1 10874 -1 1045 1005 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/27/2019 03:39:51 - INFO - __main__ -   *** Example ***
10/27/2019 03:39:51 - INFO - __main__ -   guid: train-2
10/27/2019 03:39:51 - INFO - __main__ -   tokens: [CLS] patch ##y combination of soap opera , low - tech magic realism and , at times , pl ##od ##ding ##ly sociological commentary . [SEP]
10/27/2019 03:39:51 - INFO - __main__ -   init_ids: 101 8983 2100 5257 1997 7815 3850 1010 2659 1011 6627 3894 15650 1998 1010 2012 2335 1010 20228 7716 4667 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/27/2019 03:39:51 - INFO - __main__ -   input_ids: 101 8983 2100 5257 1997 7815 3850 1010 103 1011 6627 3894 103 1998 1010 2012 2335 1010 20228 103 103 2135 24846 8570 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/27/2019 03:39:51 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/27/2019 03:39:51 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/27/2019 03:39:51 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 -1 -1 -1 -1 2659 -1 -1 -1 15650 -1 -1 -1 -1 -1 -1 7716 4667 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/27/2019 03:39:56 - INFO - __main__ -   ***** Running training *****
10/27/2019 03:39:56 - INFO - __main__ -     Num examples = 8635
10/27/2019 03:39:56 - INFO - __main__ -     Batch size = 32
10/27/2019 03:39:56 - INFO - __main__ -     Num steps = 2428
constract vocabulary based on frequency
# train data: 8635
# test  data: 960
# vocab: 20399
# class: 2
before augment best acc:0.766635
Epoch:   0%|          | 0/9 [00:00<?, ?it/s]constract vocabulary based on frequency
# train data: 25844
# test  data: 960
# vocab: 24771
# class: 2
epoch 0 augment best acc:0.773196
Epoch:  11%|█         | 1/9 [17:27<2:19:41, 1047.64s/it]constract vocabulary based on frequency
# train data: 25917
# test  data: 960
# vocab: 24236
# class: 2
epoch 1 augment best acc:0.771321
Epoch:  22%|██▏       | 2/9 [29:19<1:50:28, 946.93s/it] constract vocabulary based on frequency
# train data: 25936
# test  data: 960
# vocab: 24282
# class: 2
epoch 2 augment best acc:0.761949
Epoch:  33%|███▎      | 3/9 [40:41<1:26:44, 867.40s/it]constract vocabulary based on frequency
# train data: 25868
# test  data: 960
# vocab: 24291
# class: 2
epoch 3 augment best acc:0.771321
Epoch:  44%|████▍     | 4/9 [52:01<1:07:36, 811.32s/it]constract vocabulary based on frequency
# train data: 25906
# test  data: 960
# vocab: 24131
# class: 2
epoch 4 augment best acc:0.761949
Epoch:  56%|█████▌    | 5/9 [1:01:47<49:34, 743.58s/it]constract vocabulary based on frequency
# train data: 25914
# test  data: 960
# vocab: 24167
# class: 2
epoch 5 augment best acc:0.773196
Epoch:  67%|██████▋   | 6/9 [1:11:50<35:04, 701.40s/it]constract vocabulary based on frequency
# train data: 25891
# test  data: 960
# vocab: 24090
# class: 2
epoch 6 augment best acc:0.772259
Epoch:  78%|███████▊  | 7/9 [1:29:10<26:45, 802.93s/it]Traceback (most recent call last):
  File "aug_dataset.py", line 396, in <module>
    main()
  File "aug_dataset.py", line 286, in main
    run_aug(args, save_every_epoch=False)
  File "aug_dataset.py", line 373, in run_aug
    predictions = model(init_ids, segment_ids, input_mask)
  File "/workspace/miniconda3/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/workspace/miniconda3/envs/py35/lib/python3.5/site-packages/pytorch_pretrained_bert/modeling.py", line 862, in forward
    output_all_encoded_layers=False)
  File "/workspace/miniconda3/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/workspace/miniconda3/envs/py35/lib/python3.5/site-packages/pytorch_pretrained_bert/modeling.py", line 733, in forward
    output_all_encoded_layers=output_all_encoded_layers)
  File "/workspace/miniconda3/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/workspace/miniconda3/envs/py35/lib/python3.5/site-packages/pytorch_pretrained_bert/modeling.py", line 406, in forward
    hidden_states = layer_module(hidden_states, attention_mask)
  File "/workspace/miniconda3/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/workspace/miniconda3/envs/py35/lib/python3.5/site-packages/pytorch_pretrained_bert/modeling.py", line 392, in forward
    intermediate_output = self.intermediate(attention_output)
  File "/workspace/miniconda3/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/workspace/miniconda3/envs/py35/lib/python3.5/site-packages/pytorch_pretrained_bert/modeling.py", line 365, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/workspace/miniconda3/envs/py35/lib/python3.5/site-packages/pytorch_pretrained_bert/modeling.py", line 124, in gelu
    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
RuntimeError: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 22.38 GiB total capacity; 6.03 GiB already allocated; 20.06 MiB free; 70.00 KiB cached)

If you suspect this is an IPython bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

Epoch:  78%|███████▊  | 7/9 [1:30:48<25:56, 778.41s/it]
