Namespace(bert_model='bert-base-uncased', data_dir='datasets', do_lower_case=False, gpu=7, learning_rate=5e-05, max_seq_length=64, num_train_epochs=9.0, output_dir='aug_data_3_4_7_1.0', sample_num=3, sample_ratio=4, seed=42, task_name='TREC', temp=1.0, train_batch_size=32, warmup_proportion=0.1)
10/26/2019 15:47:25 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.
10/26/2019 15:47:26 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/26/2019 15:47:27 - INFO - __main__ -   *** Example ***
10/26/2019 15:47:27 - INFO - __main__ -   guid: train-1
10/26/2019 15:47:27 - INFO - __main__ -   tokens: [CLS] how long is human ge ##station ? [SEP]
10/26/2019 15:47:27 - INFO - __main__ -   init_ids: 101 2129 2146 2003 2529 16216 20100 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 15:47:27 - INFO - __main__ -   input_ids: 101 2129 2146 2003 2529 16216 20100 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 15:47:27 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 15:47:27 - INFO - __main__ -   segment_ids: 5 5 5 5 5 5 5 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 15:47:27 - INFO - __main__ -   masked_lm_labels: -1 -1 -1 -1 -1 -1 -1 1029 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 15:47:27 - INFO - __main__ -   *** Example ***
10/26/2019 15:47:27 - INFO - __main__ -   guid: train-2
10/26/2019 15:47:27 - INFO - __main__ -   tokens: [CLS] who is known as ` ` the world ' s oldest teenager ' ' ? [SEP]
10/26/2019 15:47:27 - INFO - __main__ -   init_ids: 101 2040 2003 2124 2004 1036 1036 1996 2088 1005 1055 4587 10563 1005 1005 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 15:47:27 - INFO - __main__ -   input_ids: 101 103 2003 2124 103 1036 1036 1996 2088 1005 1055 4587 10563 103 1005 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 15:47:27 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 15:47:27 - INFO - __main__ -   segment_ids: 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/26/2019 15:47:27 - INFO - __main__ -   masked_lm_labels: -1 2040 -1 -1 2004 -1 -1 -1 -1 -1 -1 -1 -1 1005 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
10/26/2019 15:47:28 - INFO - __main__ -   ***** Running training *****
10/26/2019 15:47:28 - INFO - __main__ -     Num examples = 4906
10/26/2019 15:47:28 - INFO - __main__ -     Batch size = 32
10/26/2019 15:47:28 - INFO - __main__ -     Num steps = 1379
constract vocabulary based on frequency
# train data: 4906
# test  data: 546
# vocab: 8680
# class: 6
before augment best acc:0.894
Epoch:   0%|          | 0/9 [00:00<?, ?it/s]constract vocabulary based on frequency
# train data: 19487
# test  data: 546
# vocab: 11519
# class: 6
epoch 0 augment best acc:0.892
Epoch:  11%|█         | 1/9 [11:27<1:31:40, 687.53s/it]constract vocabulary based on frequency
# train data: 19640
# test  data: 546
# vocab: 10583
# class: 6
epoch 1 augment best acc:0.89
Epoch:  22%|██▏       | 2/9 [22:07<1:18:33, 673.31s/it]constract vocabulary based on frequency
# train data: 19629
# test  data: 546
# vocab: 10372
# class: 6
epoch 2 augment best acc:0.89
Epoch:  33%|███▎      | 3/9 [30:45<1:02:40, 626.75s/it]constract vocabulary based on frequency
# train data: 19639
# test  data: 546
# vocab: 10243
# class: 6
epoch 3 augment best acc:0.89
Epoch:  44%|████▍     | 4/9 [40:39<51:24, 616.96s/it]  constract vocabulary based on frequency
# train data: 19632
# test  data: 546
# vocab: 10156
# class: 6
epoch 4 augment best acc:0.904
Epoch:  56%|█████▌    | 5/9 [51:34<41:52, 628.15s/it]constract vocabulary based on frequency
# train data: 19603
# test  data: 546
# vocab: 10161
# class: 6
epoch 5 augment best acc:0.886
Epoch:  67%|██████▋   | 6/9 [1:02:03<31:25, 628.44s/it]constract vocabulary based on frequency
# train data: 19636
# test  data: 546
# vocab: 10105
# class: 6
epoch 6 augment best acc:0.894
Epoch:  78%|███████▊  | 7/9 [1:10:39<19:49, 594.69s/it]constract vocabulary based on frequency
# train data: 19634
# test  data: 546
# vocab: 10165
# class: 6
epoch 7 augment best acc:0.904
Epoch:  89%|████████▉ | 8/9 [1:17:17<08:55, 535.65s/it]constract vocabulary based on frequency
# train data: 19638
# test  data: 546
# vocab: 10062
# class: 6
epoch 8 augment best acc:0.902
Epoch: 100%|██████████| 9/9 [1:24:33<00:00, 505.99s/it]Epoch: 100%|██████████| 9/9 [1:24:33<00:00, 563.76s/it]
